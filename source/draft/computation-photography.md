---
title: è¨ˆç®—æ”å½±å­¸ (Computational Photography) ç°¡ä»‹
date: 2021-06-03 14:00:00
tags: [computational photography, ]
des: ""
---

> æœ¬æ–‡ç¿»è­¯è‡ª Vasily Zubarev æ‰€å¯«çš„ã€Œ[Computational Photography-From Selfies to Black Holes](https://vas3k.com/blog/computational_photography/)ã€ã€‚å¤§éƒ¨åˆ†å…§å®¹è¡·æ–¼å‘ˆç¾åŸæ„ï¼Œå°‘éƒ¨åˆ†æœƒæ ¹æ“šè­¯è€…å°åŸæ–‡è©®é‡‹ç¨ä½œä¿®æ”¹ï¼Œå…§æ–‡ä¸­çš„ã€Œæˆ‘ã€çš†æ˜¯æŒ‡åŸä½œè€…ï¼Œè€Œè­¯è€…çš„è¨»è§£æœƒç‰¹åˆ¥æ¨™ç¤ºã€Œè­¯æŒ‰ã€ã€‚

ç¾ä»Šæ™ºæ…§æ‰‹æ©Ÿä¹‹æ‰€ä»¥å¯ä»¥é€™éº¼æ™®éæµè¡Œï¼Œå¾ˆå¤§ä¸€éƒ¨åˆ†éœ€è¦æ­¸åŠŸæ–¼æ­è¼‰ä»–å€‘ä¸Šé¢çš„ç›¸æ©Ÿã€‚Pixel å¯ä»¥åœ¨å…¨é»‘çš„æƒ…æ³ä¸‹æœ‰å¥½æ•ˆæœï¼Œè¯ç‚ºæ‰‹æ©Ÿæ”¾å¤§å°±åƒä½¿ç”¨é›™ç­’æœ›é é¡ï¼Œä¸‰æ˜Ÿæ‰‹æ©Ÿæ­è¼‰äº†å…«é¢é¡ç‰‡ï¼ŒiPhone çš„æœ¬èº«ç”šè‡³å°±è®“ä½ åœ¨æœ‹å‹ä¸­å„ªè¶Šäº†ä¸å°‘ã€‚åœ¨é€™äº›æ‰‹æ©Ÿç›¸æ©Ÿçš„èƒŒå¾Œï¼Œå…¶å¯¦è—æœ‰è‘—ä¸å¯æ€è­°çš„å‰µæ–°ã€‚

ç›¸å°åœ°ï¼Œå–®çœ¼ç›¸æ©Ÿ (DSLRs) ä¼¼ä¹æ¼¸æ¼¸å¼å¾®ã€‚å„˜ç®¡ Sony æ¯å¹´ä»æ¨é™³å‡ºæ–°ä½¿äººé©šè‰·ï¼Œä½†è£½é€ å•†çš„æ›´æ–°æ•¸åº¦é¡¯ç„¶æŒçºŒæ¸›ç·©ï¼Œç”šè‡³ä¸»è¦çš„ç‡Ÿæ”¶ä¾†æºåªå‰©ä¸‹é‚£äº›å½±ç‰‡å‰µä½œè€…ã€‚

> è­¯æŒ‰ï¼šä¸é Sony ç¢ºå¯¦é‚„æ˜¯æ»¿å²å®³çš„ï¼Œå¯ä»¥çœ‹ä»–ä»Šå¹´ (2021) æœ€æ–°ç™¼è¡¨é«˜éšæ——è‰¦æ©Ÿ Sony A1 æŠ€è¡“ç´°ç¯€ï¼š[Sony A1 çµ•å°å½±åƒç‹è€… é ‚å°–æŠ€è¡“åŠ›çš„å±•ç¾](https://www.mobile01.com/topicdetail.php?f=254&t=6311416)

æˆ‘è‡ªå·±å°±æœ‰ä¸€å°ç¾é‡‘ $3000 çš„å°¼åº·ç›¸æ©Ÿï¼Œä½†æ¯ç•¶æˆ‘æ—…è¡Œæ™‚ä»ç„¶ç”¨ iPhone ä¾†æ‹ç…§ï¼Œé€™æ˜¯ç‚ºç”šéº¼ï¼Ÿ

æˆ‘åœ¨ç¶²è·¯ä¸Šå°‹æ‰¾é€™å€‹å•é¡Œçš„ç­”æ¡ˆï¼Œæˆ‘ç™¼ç¾å¾ˆå¤šæœ‰é—œã€Œæ¼”ç®—æ³•ã€èˆ‡ã€Œç¥ç¶“ç¶²è·¯ã€çš„è¨è«–ï¼Œä½†å»æ²’äººå¯ä»¥æ¸…æ¥šåœ°è§£é‡‹é€™äº›æŠ€è¡“åˆ°åº•æ˜¯æ€æ¨£å»å½±åƒä¸€å¼µç…§ç‰‡çš„å‘ˆç¾ã€‚æ–°èè¨˜è€…åƒ…åƒ…åªæ˜¯æŠŠä¸€äº›ç”¢å“è¦æ ¼çš„æ•¸æ“šå¯«å‡ºä¾†ï¼Œéƒ¨è½å®¢åªæ˜¯ç¹¼çºŒç”¢ç”Ÿæ›´å¤šé–‹ç®±æ–‡ï¼Œç›¸æ©Ÿç‹‚ç†±è€…åªæœƒåœ¨æ„ç›¸æ©Ÿå‘ˆç¾çš„é¡è‰²å“è³ªæ˜¯å¦æ»¿æ„ã€‚å™¢ï¼ç¶²è·¯å‘€ï¼Œä½ çµ¦æˆ‘å€‘çœŸå¤šè³‡è¨Šï¼ŒçœŸæ„›ä½ ã€‚

æ–¼æ˜¯ï¼Œæˆ‘èŠ±äº†å¤§åŠè¼©å­å»äº†è§£é€™èƒŒå¾Œçš„ç¨®ç¨®åŸç†ï¼Œæˆ‘å°‡åœ¨é€™ç¯‡æ–‡è§£é‡‹æˆ‘é—œæ–¼æ‰‹æ©Ÿç›¸æ©ŸèƒŒå¾Œçš„æ‰€æœ‰äº‹æƒ…ï¼Œä¸ç„¶æˆ‘å¤§æ¦‚ä¹Ÿä¸ä¹…å°±å¿˜å…‰å…‰äº†å§ï¼

## ç”šéº¼æ˜¯è¨ˆç®—æ”å½±å­¸

é—œæ–¼è¨ˆç®—æ”å½±å­¸ (Computational Photography)ï¼Œä»»ä½•åœ°æ–¹ï¼ŒåŒ…å«[ç¶­åŸºç™¾ç§‘](https://en.wikipedia.org/wiki/Computational_photography)ï¼Œä½ å°‡æœƒå¾—åˆ°é€™æ¨£çš„å®šç¾©ï¼šã€Œè¨ˆç®—æ”å½±å­¸æ˜¯æ¡ç”¨æ•¸ä½è¨ˆç®—çš„æ–¹å¼ä¾†ç”¢ç”Ÿæ•¸ä½å½±åƒæˆ–æ˜¯å½±åƒè™•ç†ï¼Œè€Œéé€éå…‰å­¸éç¨‹ä¾†é”åˆ°ã€‚ã€åŸºæœ¬ä¸Šå¤§éƒ¨åˆ†éƒ½æŒºæ­£ç¢ºçš„ï¼Œé™¤äº†å°‘æ•¸ä¸€äº›åœ°æ–¹ï¼Œåƒæ˜¯è¨ˆç®—æ”å½±å­¸ç”šè‡³åŒ…å«è‡ªå‹•å°ç„¦çš„éƒ¨åˆ†ï¼Œä¹Ÿä¸åŒ…å«å¸¶ä¾†å¾ˆå¤šå¥½è™•çš„[å…‰å ´ç›¸æ©Ÿ](https://en.wikipedia.org/wiki/Light-field_camera)ã€‚çœ‹ä¾†å®˜æ–¹å®šç¾©ä»ç„¶æœ‰äº›æ¨¡ç³Šçš„éƒ¨åˆ†ï¼Œæˆ‘å€‘ä»ç„¶é‚„æ˜¯ä¸å¤ªæ‡‚åˆ°åº•ç”šéº¼æ˜¯è¨ˆç®—æ”å½±å­¸ï¼

> å…‰å ´ç›¸æ©Ÿæ˜¯ä¸€ç¨®æ•æ‰æ™¯ç‰©æ‰€å½¢æˆå…‰å ´è³‡è¨Šçš„ç›¸æ©Ÿï¼Œé™¤äº†è¨˜éŒ„ä¸åŒä½ç½®ä¸‹å…‰çš„å¼·åº¦åŠé¡è‰²å¤–ï¼Œä¹Ÿè¨˜éŒ„ä¸åŒä½ç½®ä¸‹å…‰ç·šçš„æ–¹å‘ï¼Œè€Œä¸€èˆ¬çš„ç›¸æ©Ÿåªèƒ½è¨˜éŒ„ä¸åŒä½ç½®ä¸‹å…‰çš„å¼·åº¦ã€‚

å²ä¸¹ä½›å¤§å­¸çš„ Marc Levoy æ•™æˆæ˜¯è¨ˆç®—æ”å½±å­¸çš„å…ˆé©…ï¼Œç›®å‰ä»–æ­£æŠ•å…¥ Google Pixel ç›¸æ©Ÿçš„é–‹ç™¼ä¸­ï¼Œåœ¨ä»–çš„[æ–‡ç« ä¸­](https://medium.com/hd-pro/a25d34f37b11)æå‡ºäº†å¦ä¸€ç¨®è§£é‡‹ï¼šã€Œè¨ˆç®—å½±åƒæŠ€è¡“ä½¿æˆ‘å€‘åŠ å¼·å’Œå»¶ä¼¸æ•¸ä½æ”å½±çš„å¯è¡Œæ€§ï¼Œä½¿å¾—æˆ‘å€‘æ‹å‡ºçš„ç…§ç‰‡çœ‹èµ·ä¾†æ˜¯å¦‚æ­¤çš„å¹³å¸¸ï¼Œå»å¹¾ä¹ä¸å¯èƒ½ä½¿ç”¨å‚³çµ±ç›¸æ©Ÿè¾¦åˆ°ã€‚ã€æˆ‘æ›´è´ŠåŒé€™å€‹å®šç¾©ï¼Œåœ¨æ¥ä¸‹ä¾†çš„æ–‡ç« ï¼Œæˆ‘å°‡éµå¾é€™å€‹å®šç¾©ã€‚

å› æ­¤ï¼Œæ™ºæ…§æ‰‹æ©Ÿæ˜¯ä¸€åˆ‡çš„æ ¹æºâ€”â€”æ™ºæ…§æ‰‹æ©Ÿåˆ¥ç„¡é¸æ“‡å¸¶çµ¦äººå€‘å…¨æ–°çš„æ”å½±æŠ€è¡“ï¼šè¨ˆç®—æ”å½±ã€‚

æ™ºæ…§æ‰‹æ©ŸåŒ…å«å¸¶æœ‰é›œè¨Šçš„æ„Ÿå…‰å…ƒä»¶å’Œæ¯”è¼ƒç²—ç³™çš„é¡é ­ã€‚æ ¹æ“šç‰©ç†å®šå¾‹ï¼Œä»–å€‘æ‡‰è©²åªæœƒå¸¶çµ¦æˆ‘å€‘ç³Ÿé€çš„å½±åƒï¼Œä½†æ˜¯ç›´åˆ°ä¸€äº›é–‹ç™¼äººå“¡ç™¼ç¾å¦‚ä½•å»æ‰“ç ´ç‰©ç†é™åˆ¶ï¼šæ›´å¿«çš„é›»å­å¿«é–€ã€å¼·å¤§çš„è™•ç†å™¨ä»¥åŠæ›´å¥½çš„è»Ÿé«”ã€‚

 ![](https://i.vas3k.ru/88h.jpg) 

é—œæ–¼è¨ˆç®—æ”å½±å­¸ï¼Œå¤§éƒ¨åˆ†é‡è¦çš„ç ”ç©¶å¤šæ•¸åœ¨ 2005-2015 å¹´ï¼Œä¸éé€™äº›éƒ½æ˜¯éå»çš„ç§‘å­¸äº†ã€‚ç¾åœ¨ï¼Œæ˜ å…¥å’±å€‘çœ¼ç°¾ä»¥åŠæ”¾åœ¨å£è¢‹çš„ï¼Œå°‡æ˜¯å‰æ‰€æœªæœ‰å¶„æ–°çš„æŠ€è¡“èˆ‡çŸ¥è­˜ã€‚

![](https://i.vas3k.ru/87c.jpg)  

è¨ˆç®—æ”å½±å­¸ä¸åƒ…åƒ…æ˜¯ HDR æˆ–å¤œé–“è‡ªæ‹æ¨¡å¼ã€‚è¿‘æœŸçš„é»‘æ´æ‹æ”å¦‚æœæ²’æœ‰æœ€æ–°çš„è¨ˆç®—æ”å½±æ–¹æ³•ï¼Œæ˜¯çµ•å°ä¸å¯èƒ½æ‹æ”å‡ºä¾†çš„ã€‚å¦‚æœè¦ç”¨ä¸€èˆ¬çš„æœ›é é¡å»æ‹é»‘æ´ï¼Œæˆ‘å€‘å°‡æœƒéœ€è¦æ•´å€‹åœ°çƒé€™éº¼å¤§çš„é¡ç‰‡ã€‚ä½†æ˜¯ï¼Œè—‰ç”±æ”¾ç½®åœ¨åœ°çƒä¸åŒè™•çš„å…«å€‹é›»æ³¢æœ›é é¡ï¼Œä¸¦ä¸”ç¶“ç”±ä¸€äº›[æ»¿é…·çš„ Python ç¨‹å¼ç¢¼](https://achael.github.io/_pages/imaging/)ï¼Œæˆ‘å€‘å¾—åˆ°äº†ä¸–ç•Œä¸Šç¬¬ä¸€å¼µäº‹ä»¶è¦–ç•Œ(event horizon)çš„ç…§ç‰‡ã€‚

ä¸éæ‹¿ä¾†æ‹è‡ªæ‹é‚„æ˜¯å¾ˆå¥½ç”¨å•¦ï¼Œä¸ç”¨å¤ªæ“”å¿ƒã€‚

ğŸ“ [Computational Photography: Principles and Practice](http://alumni.media.mit.edu/~jaewonk/Publications/Comp_LectureNote_JaewonKim.pdf)
ğŸ“ [Marc Levoy: New Techniques in Computational photography](https://graphics.stanford.edu/talks/compphot-publictalk-may08.pdf)

> é€™ç¯‡æ–‡ç« ä¸­å°‡æœƒç©¿æ’ä¸€äº›é€£çµï¼Œä»–å€‘å°‡å°å‘ä¸€äº›æˆ‘ç™¼ç¾å¾ˆæ£’çš„æ–‡ç«  ğŸ“æˆ–å½±ç‰‡ ğŸ¥ï¼Œä½¿ä½ å¯ä»¥æ›´æ·±å…¥å»äº†è§£å…¶ä¸­ä½ æœ‰èˆˆè¶£çš„éƒ¨åˆ†ï¼Œç•¢ç«Ÿæˆ‘ç„¡æ³•åœ¨çŸ­çŸ­çš„æ–‡ç« ä¸­è§£é‡‹æ‰€æœ‰æ±è¥¿ã€‚

## èµ·æºï¼šæ•¸ä½è™•ç† (Digital Processing)

å›åˆ° 2010 å¹´ï¼ŒJustin Bieber ç™¼è¡¨ä»–ç¬¬ä¸€å¼µå°ˆè¼¯ï¼Œå“ˆé‡Œç™¼å¡”(Burj Khalif)å‰›å‰›åœ¨æœæ‹œå•Ÿç”¨ï¼Œç•¶æ™‚æˆ‘å€‘é‚„æ²’æœ‰èƒ½è€å¯ä»¥å»è¨˜éŒ„ä¸‹é‚£äº›å£¯è§€çš„å®‡å®™ç¾è±¡ï¼Œå› ç‚ºæˆ‘å€‘çš„ç…§ç‰‡æ˜¯å……æ»¿å™ªéŸ³çš„å…©ç™¾è¬åƒç´  JEPGã€‚æˆ‘å€‘æœ‰äº†ç¬¬ä¸€å€‹ç„¡æ³•æŠµæŠ—çš„é¡˜æœ›ï¼Œæ˜¯é€éä½¿ç”¨ã€Œç¶“å…¸çš„(Vintage)ã€åœ–ç‰‡æ¿¾é¡ä¾†éš±è—æ‰‹æ©Ÿç›¸æ©Ÿçš„ä¸€æ–‡ä¸å€¼ï¼Œé€™æ™‚ Instagram ä¹Ÿå‡ºç¾äº†ã€‚

 ![](https://i.vas3k.ru/88i.jpg) 

# æ•¸å­¸èˆ‡ Instagram

ç”±æ–¼ Instagram çš„ç™¼å¸ƒï¼Œä»»ä½•äººå¯ä»¥è¼•é¬†åœ°ä½¿ç”¨ç…§ç‰‡æ¿¾é¡ã€‚èº«ç‚ºä¸€å€‹åŸºæ–¼ã€Œç ”ç©¶ç›®çš„ã€å°æ›¾å° X-Pro IIã€Lo-Fiã€Valencia (éƒ½æ˜¯æ¿¾é¡åç¨±) åšé€†å‘å·¥ç¨‹çš„ç”·äººï¼Œæˆ‘é‚„è¨˜å¾—é€™äº›æ¿¾é¡åŸºæœ¬ä¸ŠåŒ…å«ä¸‰å€‹éƒ¨åˆ†ï¼š

- é¡è‰²è¨­å®š(è‰²èª¿ã€é£½å’Œåº¦ã€äº®åº¦ã€å°æ¯”ã€è‰²éšç­‰)æ˜¯åŸºæœ¬çš„åƒæ•¸ï¼Œå¦‚åŒéå»æ”å½±å¸«åœ¨æ—©æœŸä½¿ç”¨çš„æ¿¾é¡ä¸€èˆ¬
![](https://i.vas3k.ru/85k.jpg) 


- è‰²èª¿æ˜ å°„(Tone Mapping)åŒ…å«ä¸€çµ„å‘é‡çš„å€¼ï¼Œä¾‹å¦‚ä»–å‘Šè¨´æˆ‘å€‘ã€Œä¸€å€‹è‰²èª¿ 128 çš„ç´…è‰²æ‡‰è©²è¢«æ”¹æˆè‰²èª¿ 240ã€ã€‚ä»–é€šå¸¸è¢«ä½¿ç”¨åœ¨å–®ä¸€é¡è‰²çš„åœ–ç‰‡ä¸­ï¼Œå¦‚åŒ[é€™å€‹](https://github.com/danielgindi/Instagram-Filters/blob/master/InstaFilters/Resources_for_IF_Filters/xproMap.png)ï¼Œæ˜¯ä¸€å€‹ X-Pro II æ¿¾é¡çš„ç¯„ä¾‹ã€‚
![](https://i.vas3k.ru/85i.jpg) 


- ç–ŠåŠ (Overlay)â€”â€”ä½¿ç”¨åŠé€æ˜çš„åœ–ç‰‡ï¼Œä¸Šé¢åŒ…å«ç°å¡µã€é¡†ç²’ã€å°æ’åœ–æˆ–æ˜¯ä»»ä½•æ±è¥¿ï¼Œä½¿å…¶è¦†è“‹åœ¨åˆ¥çš„åœ–ç‰‡ä¸Šå¾—åˆ°æ–°çš„æ•ˆæœï¼Œä¸éä¸å¸¸ä½¿ç”¨ã€‚
![](https://i.vas3k.ru/85t.jpg)  

ç¾ä»£æ¿¾é¡ä¸åƒ…åªæœ‰ä¸Šè¿°ä¸‰å€‹åƒæ•¸ï¼Œä½†å°±æœƒåœ¨æ•¸å­¸æ–¹é¢è®Šå¾—æ›´è¤‡é›œä¸€äº›ã€‚è—‰ç”±æ‰‹æ©Ÿæ”¯æ´ç¡¬é«” Shader (è‘—è‰²å™¨) è¨ˆç®—èˆ‡ [OpenCL](https://en.wikipedia.org/wiki/OpenCL) çš„æ”¯æ´ï¼Œé€™äº›è¨ˆç®—å¯ä»¥è¼•é¬†çš„åœ¨ GPU ä¸Šé¢å¯¦ç¾ï¼Œç¢ºå¯¦æ˜¯æœ‰å¤ é…·ï¼ç•¶ç„¶ï¼Œé€™æˆ‘å€‘åœ¨ 2012 å¹´æ™‚å°±èƒ½åšåˆ°ã€‚è€Œç¾åœ¨ï¼Œä»»ä½•ä¸€å€‹å­©å­éƒ½å¯ä»¥è¼•é¬†[é€é CSS](https://una.im/CSSgram/)ä¾†è¾¦åˆ°ä¸€æ¨£çš„æ•ˆæœï¼Œä¸éä»–ä»ç„¶æ²’æ©Ÿæœƒé‚€è«‹ä¸€å€‹å¥³å­©å»èˆæœƒå°±æ˜¯äº†ã€‚

ç„¶è€Œï¼Œåœ¨æ¿¾é¡ä¸Šçš„é€²å±•ä»ç„¶æŒçºŒé€²è¡Œè‘—ï¼Œåƒæ˜¯ä¸€äº›äººåœ¨ [Dehancer](http://blog.dehancer.com/category/examples/) ä¸Šå°±å¯¦è¸äº†ä¸€äº›éç·šæ€§çš„æ¿¾é¡ï¼Œä¸åŒæ–¼ç°¡å–®çš„æ˜ å°„å°æ‡‰çš„ä¿®æ”¹æ–¹å¼ï¼Œé€™äº›äººç”¨äº†ä¸å°‘è¯éº—ä¸”è¤‡é›œçš„è½‰æ›å‡½æ•¸ (transformations)ï¼Œé€™ä½¿å¾—æ¿¾é¡å¾—ä»¥æœ‰æ›´å¤šçš„å¯è¡Œæ€§ã€‚

ä½ å¯ä»¥è—‰ç”±éç·šæ€§çš„è½‰æ›å‡½æ•¸ä¾†åšåˆ°éå¸¸å¤šçš„è®ŠåŒ–ï¼Œä½†æ˜¯å°±æœƒä½¿å¾—è™•ç†è®Šå¾—è¤‡é›œï¼Œè€Œäººå€‘ä¸¦ä¸æ“…é•·é€™ç¨®è¤‡é›œçš„å·¥ä½œï¼Œå¹¸é‹çš„æ˜¯æˆ‘å€‘å¯ä»¥è—‰ç”±æ•¸å€¼æ–¹æ³•æˆ–æ˜¯ç¥ç¶“ç¶²è·¯ä¾†åšåˆ°ï¼Œä»–å€‘åšåˆ°ä¸€æ¨£çš„äº‹æƒ…ï¼Œä½†å»ç°¡å–®äº†è¨±å¤šï¼

## è‡ªå‹•åŒ–èª¿æ•´èˆ‡ã€Œä¸€éµå®Œæˆã€çš„å¤¢æƒ³

ç•¶å¤§å®¶éƒ½ç¿’æ…£ä½¿ç”¨æ¿¾é¡ä¹‹å¾Œï¼Œæˆ‘å€‘ç”šè‡³ç›´æ¥æŠŠæ¿¾é¡ç›´æ¥æ•´åˆé€²ç›¸æ©Ÿè£¡é¢äº†ã€‚èª°æ˜¯ç¬¬ä¸€å€‹æƒ³åˆ°æŠŠæ¿¾é¡æ”¾é€²ç›¸æ©Ÿçš„äººå·²ç¶“ä¸å¯è€ƒäº†ï¼Œä¸éæˆ‘å€‘å¯ä»¥å¾—çŸ¥æ—©åœ¨ iOS 5.0 ç™¼å¸ƒçš„ 2011 å¹´ï¼Œæˆ‘å€‘å·²ç¶“å¯ä»¥åœ¨è£¡é¢çœ‹åˆ°[ã€Œè‡ªå‹•å¢å¼·åœ–ç‰‡ã€å…¬é–‹çš„ API]((https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/CoreImaging/ci_autoadjustment/ci_autoadjustmentSAVE.html))ã€‚çœ‹ä¾†è³ˆä¼¯æ–¯åœ¨å…¬é–‹ API ä¹‹å‰æ—©å°±å¯Ÿè¦ºåˆ°æ¿¾é¡å·²ç¶“è¢«ä½¿ç”¨å¤šä¹…äº†ã€‚

è‡ªå‹•åŒ–èª¿æ•´åœ–ç‰‡åœ¨åšçš„äº‹æƒ…è·Ÿæˆ‘å€‘ä½¿ç”¨åœ–ç‰‡ç·¨è¼¯è»Ÿé«”åšçš„äº‹å…¶å¯¦ä¸€æ¨¡ä¸€æ¨£ï¼ŒåŸºæœ¬ä¸Šå°±æ˜¯ä¿®æ­£å…‰ç·šå’Œé™°å½±ï¼Œå¢åŠ ä¸€äº›äº®åº¦ï¼Œç§»é™¤ç´…çœ¼ï¼Œä¿®æ­£è‡‰éƒ¨çš„é¡è‰²ç­‰ï¼Œè€Œä½¿ç”¨è€…æ ¹æœ¬ä¸æœƒæƒ³åˆ°é€™å€‹ã€Œç¥å¥‡çš„åŠ å¼·ç‰ˆç›¸æ©Ÿã€å…¶å¯¦èƒŒå¾Œé çš„åƒ…åƒ…å°±æ˜¯å¹¾è¡Œçš„ç¨‹å¼ç¢¼ã€‚

 ![ML Enhance in Pixelmator](https://i.vas3k.ru/865.jpg) 

æ™‚è‡³ä»Šæ—¥ï¼Œã€Œä¸€éµç”Ÿæˆã€çš„æˆ°çˆ­å·²ç¶“è½‰ç§»è‡³æ©Ÿå™¨å­¸ç¿’çš„é ˜åŸŸäº†ã€‚å·²ç¶“å­å€¦åšä¸€å †é¢¨æ ¼è½‰æ›æ˜ å°„çš„æ“ä½œçš„äººå€‘ï¼Œé–‹å§‹è½‰å‘ [CNN å’Œ GAN çš„æ‡·æŠ±](http://vas3k.com/blog/machine_learning/)ï¼Œè®“é›»è…¦è‡ªå·±å»å¹«æˆ‘å€‘å»èª¿æ•´ä¿®åœ–æ»‘æ¡¿ã€‚æ›å¥è©±èªªï¼Œæˆ‘å€‘çµ¦æ©Ÿå™¨ä¸€å¼µåœ–ç‰‡ï¼Œä»–æœƒè‡ªå·±å»æ±ºå®šå„ç¨®å…‰å­¸çš„åƒæ•¸ï¼Œè®“ç”Ÿæˆçš„åœ–ç‰‡å»æ¥è¿‘æˆ‘å€‘æ‰€èªçŸ¥çš„ã€Œå¥½çš„ç…§ç‰‡ã€ã€‚ä½ å¯ä»¥ä¸Š Photoshop æˆ– Pixelmator Pro ä¹‹é¡çš„å½±ç‰‡ç·¨è¼¯è»Ÿé«”çš„å®˜æ–¹ç¶²é ï¼Œçœ‹çœ‹ä»–å€‘å¦‚ä½•ç”¨æœ€æ–°çš„æ©Ÿå™¨å­¸ç¿’ç‰¹è‰²åŠŸèƒ½ä¾†å¸å¼•ä½ è²·å–®ã€‚ä½ å¤§æ¦‚å¯ä»¥çŒœåˆ°æ©Ÿå™¨å­¸ç¿’ä¸æœƒæ°¸é éƒ½è¡Œçš„é€šï¼Œä½†ä½ æ°¸é å¯ä»¥é€éä½¿ç”¨ä¸€å †è³‡æ–™é›†ä¾†è¨“ç·´ä½ è‡ªå·±çš„æ©Ÿå™¨å­¸ç¿’æ¨¡å‹ä¾†åšåˆ°æ›´å¥½ã€‚ä¸‹é¢çš„ä¸€äº›è³‡æºä¹Ÿè¨±å°ä½ æœ‰å¹«åŠ©ï¼Œæˆ–æ˜¯æ²’æœ‰ XD

ğŸ“ [Image Enhancement Papers](https://paperswithcode.com/task/image-enhancement)
ğŸ“ [DSLR-Quality Photos on Mobile Devices with Deep Convolutional Networks](http://people.ee.ethz.ch/~ihnatova/#dataset)


# å †ç–Š(Stacking)ï¼šæ™ºæ…§æ‰‹æ©Ÿ 90% çš„åŠŸè‡£ 

çœŸæ­£çš„è¨ˆç®—æ”å½±å­¸ä¾†è‡ªæ–¼å †ç–Š(Stacking)â€”â€”ä¸€ç¨®å°‡å¥½å¹¾å¼µç…§ç‰‡ä¸€å¼µå¼µç–Šåœ¨ä¸€èµ·çš„æŠ€è¡“ã€‚å°æ–¼æ™ºæ…§æ‰‹æ©Ÿä¾†èªªï¼Œä¸€ç§’å…§æ‹ä¸‹å¹¾åå¼µç…§ç‰‡è¼•è€Œä¸€èˆ‰ã€‚å› ç‚ºæ‰‹æ©Ÿå…§éƒ¨çš„æ©Ÿæ§‹æ²’æœ‰ä»»ä½•ä½¿å¿«é–€è®Šæ…¢çš„éƒ¨åˆ†ï¼Œåƒæ˜¯å…‰åœˆéƒ½æ˜¯å›ºå®šçš„ï¼Œä¸¦ä¸”æ˜¯æ¡ç”¨é›»å­å¿«é–€ (ç›¸è¼ƒæ–¼å‚³çµ±çš„æ©Ÿæ¢°å¿«é–€)ã€‚è™•ç†å™¨åªéœ€è¦å„ç´ æ„Ÿæ‡‰å™¨ä»–æ‡‰è©²è¦æ”¶å¤šæ”¶æ¯«ç§’çš„å…‰å­ï¼Œç„¶å¾Œæˆ‘å€‘å°±å¾—åˆ°ä¸€å¼µç…§ç‰‡äº†ã€‚

æŠ€è¡“ä¸Šä¾†èªªï¼Œæ‰‹æ©Ÿå¯ä»¥åƒæ˜¯æ‹å½±ç‰‡ä¸€èˆ¬çš„é€Ÿåº¦å»ç…§ç›¸(æˆ‘å€‘ä¸€èˆ¬ä¸æœƒç”¨ 60fps å»ç…§ç›¸å°å§ï¼Ÿ)ï¼Œå®ƒç”šè‡³ä¹Ÿå¯ä»¥ç”¨ç…§ç‰‡çš„é«˜ç•«è³ªå»éŒ„å½±(ä¸€èˆ¬éŒ„å½± 4k (4000 è¬åƒç´ ) å·²ç¶“å¾ˆé«˜ç•«è³ªäº†ï¼Œä½†ç›¸ç‰‡å‹•è¼’ä¸€å„„ç•«ç•«ç´ )ï¼Œä½†é€™æ¨£åšéƒ½æœƒå¢åŠ è³‡æ–™å‚³è¼¸å’Œè™•ç†å™¨çš„è² æ“”ï¼Œå› æ­¤è»Ÿé«”çµ‚ç©¶æœƒå› ç‚ºç¡¬é«”è€Œæœ‰é™åˆ¶ã€‚

Stacking æŠ€è¡“å·²ç¶“ç™¼å±•æœ‰ä¸€æ®µæ™‚é–“äº†ã€‚ ç”šè‡³ç™¼æ˜è€…çš„çˆ¸çˆ¸å€‘ä¹Ÿä½¿ç”¨ Photoshop 7.0 çš„æ’ä»¶ä¾†æ”¶é›†ä¸€äº›ç˜‹ç‹‚éŠ³åŒ–çš„ HDR ç…§ç‰‡æˆ–è£½ä½œ 18000x600 åƒç´ çš„å…¨æ™¯åœ–ï¼Œè€Œä¸”â€¦æ²’äººçŸ¥é“ä¸‹ä¸€æ­¥è©²æ€éº¼åšã€‚ç¾å¥½çš„æ¢ç´¢æ™‚å…‰ã€‚

ç¾ä»Šäººå€‘å°‡å…¶ç¨±ç‚ºã€Œ[epsilon photography](https://en.wikipedia.org/wiki/Epsilon_photography) (å¾®èª¿æ”å½±)ã€ï¼Œé€™æ„å‘³è‘—æˆ‘å€‘ä¸æ–·æ›´æ”¹ç›¸æ©Ÿåƒæ•¸ï¼ˆæ›å…‰ã€èšç„¦æˆ–ä½ç½®ï¼‰ä¸¦åˆæˆå‡ºä¸€å¼µåŸæœ¬å–®é å–®æ¬¡æ‹æ”ä¸å¯èƒ½å¾—åˆ°çš„ç…§ç‰‡ã€‚ ä½†åœ¨å¯¦è¸ä¸­ï¼Œæˆ‘å€‘ç¨±é€™æŠ€è¡“ç‚ºå †ç–Šã€‚å¦‚ä»Šï¼Œæ‰€æœ‰è¡Œå‹•è£ç½®ç›¸æ©Ÿçš„å‰µæ–°ä¸­æœ‰ 90ï¼… éƒ½åŸºæ–¼æ­¤ã€‚

![](https://i.vas3k.ru/85d.jpeg) 

é›–ç„¶æœ‰å¾ˆå¤šäººä¸åœ¨ä¹ï¼Œä½†é€™å°æ–¼ç†è§£æ•´å€‹è¡Œå‹•è£ç½®æ”å½±å»è‡³é—œé‡è¦ï¼š**ç¾ä»£çš„æ™ºæ…§å‹æ‰‹æ©Ÿç›¸æ©Ÿä¸€æ‰“é–‹å°±é–‹å§‹æ‹ç…§**ã€‚ é€™æ»¿æœ‰é“ç†çš„ï¼Œç•¢ç«Ÿå®ƒè¦åœ¨è¢å¹•ä¸Šé¡¯ç¤ºåœ–åƒçµ¦ä½ çœ‹ã€‚ é™¤äº†ä¸æ–·æ‹ç…§å¤–ï¼Œå®ƒé‚„å°‡é«˜è§£æåº¦åœ–ç‰‡ä¿å­˜åœ¨ç³»çµ±çš„å¾ªç’°ç·©è¡å€ä¸­ï¼Œä¸¦å°‡å®ƒå€‘å­˜å„²å¹¾ç§’é˜ã€‚

> ç•¶æ‚¨é»æ“Šã€Œæ‹æ”ç…§ç‰‡ã€æŒ‰éˆ•æ™‚ï¼Œå¯¦éš›ä¸Šå·²ç¶“æ‰‹æ©Ÿæ—©å°±æ‹æ”äº†ç…§ç‰‡ï¼Œç›¸æ©Ÿå…¶å¯¦åªæ˜¯ä½¿ç”¨ç·©è¡å€ä¸­çš„æœ€å¾Œä¸€å¼µç…§ç‰‡

å¦‚ä»Šï¼Œé€™å°±æ˜¯ä»»ä½•æ‰‹æ©Ÿç›¸æ©Ÿçš„é‹ä½œæ–¹å¼ï¼Œè‡³å°‘é«˜éšæ™ºæ…§å‹æ‰‹æ©Ÿæ˜¯é€™æ¨£ã€‚ç·©æ²–ä¸åƒ…å¯ä»¥å¯¦ç¾é›¶[å¿«é–€å»¶é²](https://en.wikipedia.org/wiki/Shutter_lag) (æŒ‰ä¸‹å¿«é–€åˆ°çœŸçš„æ‹ä¸‹ç…§ç‰‡çš„æ™‚é–“å·®)ï¼Œé€™æ˜¯æ”å½±å¸«æœŸæœ›å·²ä¹…çš„åŠŸèƒ½ï¼Œæœ‰æ™‚ç”šè‡³å¸Œæœ›å¿«é–€å»¶é²å¯ä»¥æ˜¯è² çš„ã€‚é€éæŒ‰ä¸‹æŒ‰éˆ•ï¼Œæ‰‹æ©Ÿå¯ä»¥ç€è¦½éå»ï¼Œå¾ç·©è¡å€ä¸­å»æ’ˆ 5-10 å¼µæœ€å¾Œçš„ç…§ç‰‡ï¼Œä¸¦é–‹å§‹å°å…¶é€²è¡Œç˜‹ç‹‚åœ°åˆ†æå’Œçµ„åˆã€‚æ‰€ä»¥æˆ‘å€‘ç”šè‡³ä¸å†éœ€è¦ä½¿ç”¨é«˜å‹•æ…‹ç¯„åœæˆåƒ (HDR) æˆ–å¤œé–“æ¨¡å¼ï¼Œæ‰‹æ©Ÿè»Ÿé«”æœƒå¾ç·©è¡å€å»è™•ç†å¥½é€™äº›ç…§ç‰‡ï¼Œç”¨æˆ¶ç”šè‡³éƒ½ä¸æœƒæ„è­˜åˆ°æ‹çš„ç…§ç‰‡æ˜¯è¢«åŠ å·¥éçš„ã€‚ 

äº‹å¯¦ä¸Šï¼Œé€™å°±æ˜¯ç¾åœ¨ iPhone æˆ– Pixel åœ¨åšçš„äº‹æƒ…ã€‚

![](https://i.vas3k.ru/88j.jpg) 

## æ›å…‰ç–ŠåŠ  (Exposure Stacking)ï¼šé«˜å‹•æ…‹ç¯„åœæˆåƒ (HDR) èˆ‡å…‰ç·šæ§åˆ¶

 ![](https://i.vas3k.ru/85x.jpg) 


ä¸€å€‹å­˜åœ¨å·²ä¹…çš„ç†±é–€è©±é¡Œæ˜¯ç›¸æ©Ÿå‚³æ„Ÿå™¨æ˜¯å¦[å¯ä»¥æ•æ‰æˆ‘å€‘çœ¼ç›å¯ä»¥çœ‹è¦‹çš„æ•´å€‹äº®åº¦ç¯„åœ](https://www.cambridgeincolour.com/tutorials/cameras-vs-human-eye.htm)ã€‚ æœ‰äººèªªä¸è¡Œï¼Œå› ç‚ºçœ¼ç›æœ€å¤šå¯ä»¥çœ‹åˆ° 25 å€‹ [f-stops](https://en.wikipedia.org/wiki/F-number)ï¼Œç”šè‡³é ‚ç´šçš„å…¨ç•«å¹…å‚³æ„Ÿå™¨ä¹Ÿå°±æœ€å¤§é”åˆ° 14ã€‚å…¶ä»–äººå‰‡è¦ºå¾—ä¸æ­£ç¢ºï¼Œå› ç‚ºæˆ‘å€‘çš„çœ¼ç›æ˜¯ç”±å¤§è…¦è¼”åŠ©çš„ï¼Œå¤§è…¦æœƒè‡ªå‹•èª¿æ•´æ‚¨çš„ç³å­”ä¸¦é€šéå…¶ç¥ç¶“ç¶²çµ¡å®Œæˆåœ–åƒã€‚æ‰€ä»¥çœ¼ç›çš„ç¬æ™‚å‹•æ…‹ç¯„åœå¯¦éš›ä¸Šä¸è¶…é10-14 f-stopsã€‚å¤ªé›£äº†ï¼è®“æˆ‘å€‘æŠŠé€™äº›çˆ­è«–ç•™çµ¦ç§‘å­¸å®¶ã€‚ 

ä½†å•é¡Œä¾èˆŠå­˜åœ¨â€”ç•¶ä½ ä½¿ç”¨ä»»ä½•æ‰‹æ©Ÿæ‹æ”èƒŒå°è—å¤©çš„æœ‹å‹æ™‚ï¼Œå¦‚æœæ²’æœ‰ä½¿ç”¨ HDRï¼Œä½ è¦é¦¬å¾—åˆ°ä¸€å€‹æ¸…æ¥šçš„å¤©ç©ºä½†æœ‹å‹æ˜¯é»‘çš„ï¼Œåˆæˆ–è€…æœ‹å‹æ˜¯æ¸…æ¥šçš„ä½†å¤©ç©ºå»éæ›äº†ã€‚

äººå€‘å¾ˆä¹…ä»¥å‰å°±æ‰¾åˆ°äº†è§£æ±ºæ–¹æ¡ˆâ€”â€”ä½¿ç”¨ HDRï¼ˆé«˜å‹•æ…‹ç¯„åœï¼‰æ“´å¤§äº®åº¦ç¯„åœã€‚ç•¶æˆ‘å€‘ä¸èƒ½ç«‹å³å–å¾—å¤§ç¯„åœçš„äº®åº¦æ™‚ï¼Œæˆ‘å€‘å¯ä»¥åˆ†ä¸‰æ­¥ï¼ˆæˆ–æ›´å¤šï¼‰ä¾†å®Œæˆã€‚æˆ‘å€‘å¯ä»¥ç”¨ä¸åŒçš„æ›å…‰æ‹æ”å¹¾å¼µç…§ç‰‡â€”â€”ã€Œæ­£å¸¸ã€çš„ã€æ›´äº®çš„ã€æ›´æš—çš„å„ä¸€å¼µã€‚ç„¶å¾Œæˆ‘å€‘å¯ä»¥ç”¨æ˜äº®çš„ç…§ç‰‡å¡«å……é™°å½±çš„éƒ¨åˆ†ï¼Œä¸¦å¾è¼ƒæš—çš„ç…§ç‰‡ä¸­æ¢å¾©éåº¦æ›å…‰çš„å€åŸŸã€‚

é€™è£¡éœ€è¦åšçš„æœ€å¾Œä¸€ä»¶äº‹æ˜¯è§£æ±ºè‡ªå‹•åŒ…åœçš„å•é¡Œã€‚æˆ‘å€‘éœ€è¦å°‡æ¯å¼µç…§ç‰‡çš„æ›å…‰é‡å¦‚ä½•åˆ†é…èª¿æ•´ï¼Œä»¥å…æœ‰éåº¦æ›å…‰ï¼Ÿç„¶è€Œï¼Œä»Šå¤©ä»»ä½•ç†å·¥ç”Ÿéƒ½å¯ä»¥ä½¿ç”¨ä¸€äº› Python ç¨‹å¼ç¢¼ä¾†åšåˆ°é€™ä¸€é»ã€‚

 ![](https://i.vas3k.ru/86t.jpg) 

ç•¶æœ€æ–°æ¬¾ iPhoneã€Pixel å’Œ Galaxy ç›¸æ©Ÿå…§çš„ç°¡å–®ç®—æ³•æª¢æ¸¬åˆ°æ‚¨åœ¨æ™´å¤©æ‹æ”æ™‚ï¼Œå®ƒå€‘æœƒè‡ªå‹•é–‹å•Ÿ HDR æ¨¡å¼ã€‚æ‚¨ç”šè‡³å¯ä»¥çœ‹åˆ°æ‰‹æ©Ÿå¦‚ä½•åˆ‡æ›åˆ°ç·©æ²–æ¨¡å¼ (Buffer Mode) ä»¥ä¿å­˜ç§»å‹•çš„åœ–åƒâ€”â€”æ­¤æ™‚ FPS ä¸‹é™ï¼Œè¢å¹•ä¸Šçš„åœ–ç‰‡è®Šå¾—æ›´åŠ ç”Ÿå‹•ã€‚æ¯æ¬¡åˆ‡æ›çš„ç¬é–“åœ¨æˆ‘çš„ iPhone X ä¸Šéƒ½æ¸…æ™°å¯è¦‹ã€‚ä¸‹æ¬¡ä»”ç´°çœ‹çœ‹ä½ çš„æ™ºèƒ½æ‰‹æ©Ÿï¼

.block-side.block-side__right.width-25  ![](https://i.vas3k.ru/87u.png)  

å¸¶æœ‰åŒ…åœæ›å…‰çš„ HDR çš„ä¸»è¦ç¼ºé»æ˜¯å®ƒåœ¨å…‰ç·šä¸è¶³çš„æƒ…æ³ä¸‹ä»¤äººé›£ä»¥ç½®ä¿¡çš„æ¯«ç„¡ç”¨æ­¦ä¹‹åœ°ã€‚ å³ä½¿åœ¨å®¶ç”¨ç‡ˆçš„å…‰ç·šä¸‹ï¼Œç…§ç‰‡ä»ç„¶å¾ˆæš—ï¼Œç”šè‡³æ‰‹æ©Ÿä¹Ÿç„¡æ³•å°‡å®ƒå€‘èª¿æ•´å †ç–Šåœ¨ä¸€èµ·ã€‚ç‚ºäº†è§£æ±ºé€™å€‹å•é¡Œï¼Œè°·æ­Œæ—©åœ¨ 2013 å¹´å°±åœ¨ Nexus æ™ºèƒ½æ‰‹æ©Ÿä¸­å®£å¸ƒäº†ä¸€ç¨®ä¸åŒçš„ HDR æ–¹æ³•ã€‚å®ƒä½¿ç”¨æ™‚é–“ç–ŠåŠ  (Time Stacking)ã€‚

## æ™‚é–“ç–ŠåŠ  (Time Stacking)ï¼šé•·æ™‚æ›å…‰èˆ‡æ™‚é–“æµé€

 ![](https://i.vas3k.ru/85v.jpg) 

æ™‚é–“ç–ŠåŠ å¯è®“æ‚¨é€éä¸€ç³»åˆ—çŸ­æ™‚é–“æ›å…‰çš„ç…§ç‰‡ç²å¾—é•·æ›å…‰æ•ˆæœã€‚é€™ç¨®æ–¹æ³•æ˜¯ç”±å–œæ­¡æ‹æ”å¤œç©ºä¸­æ˜Ÿè·¡ç…§ç‰‡çš„å¤©æ–‡æ„›å¥½è€…å€‘é¦–å‰µçš„ã€‚å³ä½¿ä½¿ç”¨ä¸‰è…³æ¶ï¼Œä¹Ÿç„¡æ³•åšåˆ°æ‰“é–‹å¿«é–€å…©å€‹å°æ™‚ä¾†æ‹æ”é€™æ¨£çš„ç…§ç‰‡ã€‚æ‚¨å¿…é ˆäº‹å…ˆè¨ˆç®—æ‰€æœ‰è¨­ç½®ï¼Œä»»ä½•è¼•å¾®çš„æ™ƒå‹•éƒ½æœƒç ´å£æ•´å€‹æ‹æ”çµæœã€‚æ‰€ä»¥ä»–å€‘æ±ºå®šå°‡é€™å€‹éç¨‹åˆ†æˆå¥½å¤šå€‹å¹¾åˆ†é˜çš„ç…§ç‰‡ï¼Œç„¶å¾Œåœ¨ Photoshop ä¸­å°‡åœ–ç‰‡å †ç–Šåœ¨ä¸€èµ·ã€‚ 

 ![These star patterns are always glued together from a series of photos. That make it easier to control exposure](https://i.vas3k.ru/86u.jpg) 

å› æ­¤ï¼Œç›¸æ©Ÿå¾ä¾†æ²’æœ‰é•·æ™‚é–“æ›å…‰æ‹æ”éã€‚æˆ‘å€‘é€éçµ„åˆå¹¾å€‹é€£çºŒé¡é ­ä¾†æ¨¡æ“¬æ•ˆæœã€‚é•·æœŸä»¥ä¾†ï¼Œæ‰‹æ©Ÿä¸Šæœ‰å¾ˆå¤šæ‡‰ç”¨ç¨‹å¼éƒ½åœ¨ä½¿ç”¨é€™å€‹æŠ€å·§ï¼Œä½†ç¾åœ¨å¹¾ä¹æ¯å€‹è£½é€ å•†éƒ½å°‡å…¶æ·»åŠ åˆ°æ¨™æº–ç›¸æ©Ÿå·¥å…·ä¸­ã€‚

 ![A long exposure made of iPhone's Live Photo in 3 clicks](https://i.vas3k.ru/86f.jpg) 

è®“æˆ‘å€‘å›åˆ° Google å’Œå®ƒçš„å¤œé–“ HDRã€‚äº‹å¯¦è­‰æ˜ï¼Œä½¿ç”¨æ™‚é–“åŒ…åœå¯ä»¥åœ¨é»‘æš—ä¸­å‰µå»ºä¸€å€‹ä¸éŒ¯çš„ HDRã€‚ é€™é …æŠ€è¡“é¦–æ¬¡å‡ºç¾åœ¨ Nexus 5 ä¸­ï¼Œè¢«ç¨±ç‚º HDR+ã€‚ è©²æŠ€è¡“ä»ç„¶ååˆ†å—æ­¡è¿ï¼Œä»¥è‡³æ–¼åœ¨æœ€æ–°çš„ Pixel æ¼”ç¤ºæ–‡ç¨¿ä¸­ [å®ƒç”šè‡³å—åˆ°ç¨±è®š](https://www.youtube.com/watch?v=iLtWyLVjDg0&t=0)ã€‚ 

HDR+ çš„å·¥ä½œéå¸¸ç°¡å–®ï¼šä¸€æ—¦ç›¸æ©Ÿæª¢æ¸¬åˆ°æ‚¨åœ¨é»‘æš—ä¸­æ‹æ”ï¼Œå®ƒå°±æœƒå¾ç·©è¡å€ä¸­å–å‡ºæœ€å¾Œ 8-15 å¼µ RAW ç…§ç‰‡ä¸¦å°‡å®ƒå€‘å †ç–Šåœ¨ä¸€èµ·ã€‚é€™æ¨£ï¼Œæ¼”ç®—æ³•æœƒæ”¶é›†æ›´å¤šé—œæ–¼é¡é ­æš—å€çš„è¨Šæ¯ï¼Œä»¥æœ€å¤§é™åº¦åœ°æ¸›å°‘å™ªé»åƒç´ ï¼Œé¿å…æŸäº›åŸå› å°è‡´ç›¸æ©Ÿå‡ºéŒ¯ä¸¦ä¸”æœªèƒ½åœ¨æ¯å€‹ç‰¹å®šå¹€ä¸Šæ•æ‰åˆ°å…‰å­ã€‚

æƒ³åƒä¸€ä¸‹ï¼šä½ ä¸çŸ¥é“ [Capybara](https://en.wikipedia.org/wiki/Capybara) é•·ä»€éº¼æ¨£ï¼Œæ‰€ä»¥ä½ æ±ºå®šå•äº”å€‹äººã€‚ä»–å€‘çš„æ•…äº‹å¤§è‡´ç›¸åŒï¼Œä½†æ¯å€‹äººéƒ½æœƒæåˆ°ä»»ä½•ç¨ç‰¹çš„ç´°ç¯€ï¼Œå› æ­¤èˆ‡åƒ…è©¢å•ä¸€å€‹äººç›¸æ¯”ï¼Œæ‚¨å¯ä»¥ç²å¾—æ›´å¤šè³‡è¨Šã€‚ç…§ç‰‡ä¸Šçš„åƒç´ ä¹Ÿæœƒç™¼ç”ŸåŒæ¨£çš„æƒ…æ³ã€‚æ›´å¤šè³‡è¨Šã€æ›´æ¸…æ™°ã€å™ªé»æ›´å°‘ã€‚

ğŸ“ [HDR+: Low Light and High Dynamic Range photography in the Google Camera App](https://ai.googleblog.com/2014/10/hdr-low-light-and-high-dynamic-range.html)

çµ„åˆå¾åŒä¸€é»æ•ç²çš„åœ–åƒæœƒç”¢ç”Ÿèˆ‡ä¸Šé¢å¸¶æœ‰æ˜Ÿæ˜Ÿçš„ç¤ºä¾‹ä¸­ç›¸åŒçš„å‡é•·æ›å…‰æ•ˆæœã€‚å¹¾åå¼µç…§ç‰‡çš„æ›å…‰åŒ¯ç¸½ï¼Œä¸€å¼µç…§ç‰‡çš„éŒ¯èª¤åœ¨å¦ä¸€å¼µç…§ç‰‡ä¸Šè¢«æœ€å°åŒ–ã€‚æƒ³åƒä¸€ä¸‹ï¼Œè¦å¯¦ç¾é€™ä¸€é»ï¼Œæ‚¨éœ€è¦åœ¨æ•¸ç¢¼å–®åç›¸æ©Ÿä¸­çŒ›æŒ‰å¿«é–€å¤šå°‘æ¬¡ã€‚

 ![Pixel ad that glorifies HDR+ and Night Sight](https://i.vas3k.ru/86g.jpg) 

åªå‰©ä¸‹ä¸€ä»¶äº‹ï¼Œå°±æ˜¯è‡ªå‹•è‰²å½©ç©ºé–“æ˜ å°„ã€‚åœ¨é»‘æš—ä¸­æ‹æ”çš„ç…§ç‰‡é€šå¸¸æœƒç ´å£è‰²å½©å¹³è¡¡ï¼ˆåé»ƒæˆ–åç¶ ï¼‰ï¼Œå› æ­¤æˆ‘å€‘éœ€è¦æ‰‹å‹•ä¿®å¾©ã€‚åœ¨æ—©æœŸç‰ˆæœ¬çš„ HDR+ ä¸­ï¼Œé€™å€‹å•é¡Œæ˜¯é€éç°¡å–®çš„è‡ªå‹•è‰²èª¿ä¿®å¾©è§£æ±ºçš„ï¼Œå°±åƒ Instagram æ¿¾é¡ä¸€æ¨£ã€‚å¾Œä¾†ï¼Œä»–å€‘ä½¿ç”¨äº†ç¥ç¶“ç¶²çµ¡ä¾†å¾©åŸè‰²å½©ã€‚

[Night Sight æŠ€è¡“](https://www.blog.google/products/pixel/see-light-night-sight/) å°±æ˜¯é€™æ¨£èª•ç”Ÿçš„â€”â€”Pixel 2ã€3 å’Œæ›´é«˜ç‰ˆæœ¬ä¸­çš„ã€Œå¤œé–“æ”å½±ã€æŠ€è¡“ã€‚æè¿°èªªã€ŒHDR+ æ˜¯å»ºç«‹åœ¨æ©Ÿå™¨å­¸ç¿’æŠ€è¡“ä¹‹ä¸Šã€ã€‚äº‹å¯¦ä¸Šï¼Œå®ƒåªæ˜¯ç¥ç¶“ç¶²çµ¡å’Œæ‰€æœ‰ HDR+ å¾Œè™•ç†æ­¥é©Ÿçš„ä¸€å€‹èŠ±å“¨åç¨±ã€‚æ©Ÿå™¨æ¥å—äº†ã€Œä¹‹å‰ã€å’Œã€Œä¹‹å¾Œã€ç…§ç‰‡æ•¸æ“šé›†çš„è¨“ç·´ï¼Œä»¥å¾ä¸€çµ„é»‘æš—å’Œé­äº‚çš„ç…§ç‰‡ä¸­è£½ä½œå‡ºä¸€å¼µæ¼‚äº®çš„åœ–åƒã€‚

 ![](https://i.vas3k.ru/88k.jpg) 

é †ä¾¿èªªä¸€ä¸‹ï¼Œé€™å€‹æ•¸æ“šé›†æ˜¯å…¬é–‹çš„ã€‚ ä¹Ÿè¨±è˜‹æœå…¬å¸çš„äººæœƒæ¥å—å®ƒä¸¦æœ€çµ‚æ•™ä»–å€‘â€œä¸–ç•Œä¸Šæœ€å¥½çš„ç›¸æ©Ÿâ€åœ¨é»‘æš—ä¸­æ‹æ”ï¼Ÿ

269 / 5000
Translation results
æ­¤å¤–ï¼ŒNight Sight è¨ˆç®—é¡é ­ä¸­ç‰©é«”çš„ [é‹å‹•çŸ¢é‡](https://en.wikipedia.org/wiki/Optical_flow) ä»¥æ¨™æº–åŒ–æ¨¡ç³Šï¼Œé€™è‚¯å®šæœƒå‡ºç¾åœ¨é•·æ™‚é–“æ›å…‰ä¸­ã€‚ å› æ­¤ï¼Œæ™ºèƒ½æ‰‹æ©Ÿå¯ä»¥å¾å…¶ä»–é¡é ­ä¸­å–å‡ºé‹’åˆ©çš„éƒ¨åˆ†ä¸¦å°‡å®ƒå€‘å †ç–Šèµ·ä¾†ã€‚ .

ğŸ“ [Night Sight: Seeing in the Dark on Pixel Phones](https://ai.googleblog.com/2018/11/night-sight-seeing-in-dark-on-pixel.html ".block-link")
ğŸ“ [Introducing the HDR+ Burst Photography Dataset](https://ai.googleblog.com/2018/02/introducing-hdr-burst-photography.html ".block-link")


## Motion Stacking<br><small>Panorama, super-zoom and noise control</small>

 ![](https://i.vas3k.ru/85z.jpg) 

Panorama has always been a favorite kids toy. World's history knows no cases when a sausage photo was interesting to anyone but its author. However, it's still worth to talk about it though because that's how stacking got into many people's lives.

 ![](https://i.vas3k.ru/85c.jpg) 

The very first useful application for panorama is making super-resolution photos. By combining multiple slightly shifted images, you can get a much higher resolution image than the camera provides. Thus you can receive a photo in [hundreds of gigapixels](http://sh-meet.bigpixel.cn/?from=groupmessage&isappinstalled=0) resolution, which is very useful if you need to print it for a house-sized billboard.

ğŸ“ [A Practical Guide to Creating Superresolution Photos with Photoshop](https://petapixel.com/2015/02/21/a-practical-guide-to-creating-superresolution-photos-with-photoshop/ ".block-link")

 ![](https://i.vas3k.ru/88l.jpg) 

Another and more interesting approach called Pixel Shifting. Some mirrorless cameras like Sony and Olympus started [supporting it](https://petapixel.com/2017/11/18/testing-sonys-new-pixel-shift-feature-a7r-iii/) in 2014, but they're still asking you to combine the result yourself. Typical DSLR innovations.

Smartphones have succeeded here for a hilarious reason. When you take a picture, your hands are shaking. This "problem" became the basis for the implementation of native super-resolution on smartphones.

To understand how it works, we need to remember how any camera's sensor works. Each pixel (photodiode) can capture only the intensity of light, i.e., the number of photons which broke through. However, a pixel cannot measure the color (wavelength). In order to get an RGB-image, we had to hack it around and cover the whole sensor with a grid of multicolored glasses. Its most popular implementation is called [Bayer filter](https://en.wikipedia.org/wiki/Color_filter_array) and is used today in most sensors.

 ![](https://i.vas3k.ru/88m.jpg) 

It turns out that each pixel of the sensor catches only R, G or B-component because rest of the photons are mercilessly reflected by Bayer filter. Missing components are computed by averaging nearby pixels later.

.block-side.block-side__right  ![](https://i.vas3k.ru/863.png)  

Made by analogy with the human eye, the Bayer filter has more green cells than others. Thus, out of 50 million pixels on the sensor, about 25 million will only (!) capture the green light, while red and blue will capture 12.5 million each. The rest is averaged. This process called debayering or demosaicing and this is that fat and funny kludge which keeps everything together.

 ![](https://i.vas3k.ru/88n.jpg) 

% In fact, each sensor has its own tricky and (of course) patented demosaicing algorithm, but in this story we don't care.

Other types of sensors (such as [Foveon](https://en.m.wikipedia.org/wiki/Foveon_X3_sensor)) didn't get that popular. Some rare smartphone manufacturers like Huawei though tried to play with non-Bayer filters to improve sharpness and dynamic range. Mostly unsuccessful.

Thanks to the Bayer filter, we lose a ton of photons, especially in the dark. Thus, we came up with the idea of Pixel Shifting â€” shift the sensor by one-pixel up-down-left-right to catch them all. The photo doesn't appear to be 4 times larger, as you might think, it just helps the demosaicing algorithm do its job better â€” now it can average colors not by neighboring pixels, but by 4 versions of itself.

 ![](https://i.vas3k.ru/88o.jpg) 

.block-side.block-side__right.width-25 ![](https://3.bp.blogspot.com/-HerRsuhYbSU/W8PBOpTqgII/AAAAAAAADYU/M03RG5n11AcTpDHpRJdU_SZ0m0pVPF0YgCLcBGAs/s400/image10.gif) 

Our shaking hands make Pixel Shifting natural for mobile photography. And that's how it implemented in the latest versions of Google Pixel. You notice it when zooming on your Android phone. This zooming called Super Res Zoom (yes, I also enjoy the harsh naming). Chinese manufacturers already copied it to their phones, although it's worse than the original.

ğŸ“ [SIGGRAPH 2019: Handheld Multi-frame Super-resolution](https://sites.google.com/view/handheld-super-res ".block-link")
ğŸ“ [See Better and Further with Super Res Zoom on the Pixel 3](https://ai.googleblog.com/2018/10/see-better-and-further-with-super-res.html ".block-link")

Stacking of slightly shifted photos allows us to collect more information about every pixel to reduce noise, sharpen and raise the resolution without increasing the physical number of sensor megapixels. Modern Android phones do it automatically, while their users don't even realize.





## Focus Stacking<br><small>DoF and refocus in post-production</small>

 ![](https://i.vas3k.ru/85y.jpg) 

The method came from macro photography, where the depth of field has always been a problem. To keep the entire object in focus, you had to take several shots, moving focus back and forth, and combine them later into one sharp shot in photoshop. The same method is often used by landscape photographers to make the foreground and background sharp as shark.

 ![Focus stacking in macro. DoF is too small and you can't shoot it one go](https://i.vas3k.ru/86c.jpg) 

Of course, it all migrated to smartphones. With no hype, though. Nokia released Lumia 1020 with "Refocus App" in 2013, and Samsung Galaxy S5 did the same in 2014 with "[Selective Focus](https://recombu.com/mobile/article/focus-shifting-explained_m20454-html)". Both used the same approach â€” they quickly took 3 photos: focused one, focus shifted forth and shifted back. The camera then aligned the images and allowed you to choose one of them, which was introduced as a "real" focus control in the post-production.

There was no further processing, as even this simple hack was enough to hammer another nail in the coffin of Lytro and analogs that used a fair refocus. Let's talk about them, by the way (topic change master 80 lvl).






# Computational Sensor<br><small>Plenoptic and Light Fields</small>

Well, our sensors are shit. We simply got used to it and trying to do our best with them. They haven't changed much in their design from the beginning of time. Technical process was the only thing that improved â€” we reduced the distance between pixels, fought noise, and added specific pixels for [phase-detection autofocus system](https://www.imaging-resource.com/news/2015/09/15/sony-mirrorless-cameras-will-soon-focus-as-fast-as-dslrs-if-this-patent-bec). But even if we take the most expensive camera to try to photograph a running cat in the indoor light, the cat will win.

 ![](https://i.vas3k.ru/88p.jpg) 

ğŸ¥ [The Science of Camera Sensors](https://www.youtube.com/watch?v=MytCfECfqWc ".block-link")

.block-side.block-side__right.width-25  ![](https://i.vas3k.ru/881.jpg)  

We've been trying to invent a better sensor for a long time. You can google a lot of researches in this field by "computational sensor" or "non-Bayer sensor" queries. Even the Pixel Shifting example can be referred to as an attempt to improve sensors with calculations.

The most promising stories of the last twenty years, though, come to us from plenoptic cameras.

To calm your sense of impending boring math, I'll throw in the insider's note â€” the last Google Pixel camera is a little bit plenoptic. With only two pixels in one, there's still enough to calculate a fair optical depth of field map without having a second camera like everyone else.

Plenoptics is a powerful weapon that hasn't fired yet.





## Plenoptic Camera

.block-side.block-side__right.width-25  ![](https://i.vas3k.ru/860.jpg)  

Invented in 1994. For the first time assembled in Stanford in 2004. The first consumer product â€” Lytro, released in 2012. The VR industry is now actively experimenting with similar technologies.

Plenoptic camera differs from the normal one by only one modification. Its sensor covered with a grid of lenses, each of which covers several real pixels. Somehow like that:

 ![](https://i.vas3k.ru/89o.jpg) 

If we place the grid and sensor at the right distance, we'll see sharp pixel clusters containing mini-versions of the original image on the final RAW image.

 ![](https://i.vas3k.ru/866.jpg) 

ğŸ¥ [Muted video showing RAW editing process](https://www.youtube.com/watch?v=jT2aO3BLFRU)

Apparently, if you take only one central pixel from each cluster and build the image only from them, it won't be any different from one taken with a standard camera. Yes, we lose a bit in resolution, but we'll just ask Sony to stuff more megapixels in the next sensor.

That's where the fun part begins. If you take another pixel from each cluster and build the image again, you again get a standard photo, only as if it was taken with a camera shifted by one pixel in space. Thus, with 10x10 pixel clusters, we get 100 images from "slightly" different angles.

 ![](https://i.vas3k.ru/88r.jpg) 

The more the cluster size, the more images we have. Resolution is lower, though. In the world of smartphones with [41-megapixel](https://www.cameradebate.com/2013/nokia-lumia-1020-camera-sensor-lens/) sensors, everything has a limit, although we can neglect resolution a bit. We have to keep the balance.

ğŸ“ [plenoptic.info - about plenoptics, with python code samples](http://www.plenoptic.info ".block-link")

Alright, we've got a plenoptic camera. What can we do with it?



### Fair refocusing

The feature that everyone was buzzing about in the articles covering Lytro is a possibility to adjust focus after the shot was taken. "Fair" means we don't use any deblurring algorithms, but rather only available pixels, picking or averaging in the right order.

.block-side.block-side__right  ![](https://i.vas3k.ru/86w.jpg)  

A RAW photo taken with a plenoptic camera looks weird. To get the usual sharp JPEG out of it, you have to assemble it first. The result will vary depending on how we select the pixels from the RAW.

The farther is the cluster from the point of impact of the original ray, the more defocused the ray is. Because the optics. To get the image shifted in focus, we only need to choose the pixels at the desired distance from the original â€” either closer or farther. 

 ![The picture should be read from right to left as we are sort of restoring the image, knowing the pixels on the sensor. We get a sharp original image on top, and below we calculate what was behind it. That is, we shift the focus computationally](https://i.vas3k.ru/88s.jpg) 

The process of shifting the focus forward is a bit more complicated as we have fewer pixels in these parts of the clusters. In the beginning, Lytro developers didn't even want to let the user focus manually because of that â€” the camera made a decision itself using the software. Users didn't like that, so the feature was added in the late versions as "creative mode", but with very limited refocus for exactly that reason.




### Depth Map and 3D using a single lens

One of the simplest operations in plenoptics is to get a depth map. You just need to gather two different images and calculate how the objects are shifted at them. More shift â€” farther away from the camera.

Google recently bought and killed Lytro, but used their technology for its VR and... Pixel's camera. Starting Pixel 2, the camera became "a little bit" plenoptic, though with only two pixels per cluster. As a result, Google doesn't need to install a second camera like all the other cool kids. Instead, they can calculate a depth map from one photo.

 ![Images which left and right subpixels of Google Pixel see. The right one is animated for clarity (look closer)](https://i.vas3k.ru/full/86x.gif) 

 ![The depth map is additionally processed with neural networks to make the background blur more even](https://i.vas3k.ru/88t.jpg) 

ğŸ“ [Portrait mode on the Pixel 2 and Pixel 2 XL smartphones](https://ai.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html ".block-link")

The depth map is built on two shots shifted by one sub-pixel. This is enough to calculate a binary depth map and separate the foreground from the background to blur it out in the fashionable bokeh. The result of this stratification is still smoothed and "improved" by neural networks which are trained to improve depth maps (rather than to observe, as many people think).

 ![](https://i.vas3k.ru/88u.jpg) 

% The trick is that we got plenoptics in smartphones almost at no charge. We already [put lenses](https://i.vas3k.ru/86y.png) on these tiny sensors to increase the luminous flux at least somehow. In the next Pixel phones, Google has plans to go further and cover four photodiodes with a lens.


### Slicing layers and objects

You don't see your nose because your brain combines a final image from both of your eyes. Close one eye, and you will see a huge Egyptian pyramid at the edge.

The same effect can be achieved in a plenoptic camera. By assembling shifted images from pixels of different clusters, we can look at the object as if from several points. Same as our eyes do. It gives us two cool opportunities. First is we can estimate the approximate distance to the objects, which allows us easily separate the foreground from the background as in life. And second, if the object is small, we can completely remove it from the photo. Like a nose. Optically, for real, with no photoshop. 

Using this, we can cut out trees between the camera and the object or remove the falling confetti, as in the video below.

 ![](https://i.vas3k.ru/87a.jpg) 

### "Optical" stabilization with no optics

From a plenoptic RAW, you can make a hundred of photos with several pixels shift over the entire sensor area. Accordingly, we have a tube of lens diameter within which we can move the shooting point freely, thereby offsetting the shake of the image.

 ![](https://i.vas3k.ru/89n.jpg) 

Technically, stabilization is still optical, because we don't have to calculate anything â€” we just select pixels in the right places. On the other hand, any plenoptic camera sacrifices the number of megapixels in favor of plenoptic capabilities, and any digital stabilizer works the same way. It's nice to have it as a bonus, but using it only for its sake is doubtful.

The larger sensor and lens, the bigger window for movement. The more camera capabilities, the more ozone holes from supplying this circus with electricity and cooling. Yeah, technology!

### Fighting with Bayer filter

Bayer filter is still necessary even with a plenoptic camera. We haven't come up with any other way of getting a colorful digital image. And using a plenoptic RAW, we can average the color not only by the group of nearby pixels, as in the classic demosaicing, but also using dozens of its copies in neighboring clusters.

It's called "computable super-resolution" in some articles, but I would question it. In fact, we reduce the real resolution of the sensor in these same dozen times first in order to proudly restore it again. You have to try hard to sell it to someone.

But technically it's still more interesting than shaking the sensor in a pixel shifting spasm.

 ![](https://i.vas3k.ru/88x.jpg) 

### Computational aperture (bokeh)

.block-side.block-side__right  ![](https://i.vas3k.ru/86z.jpg)  

Those who like to shoot bokeh hearts will be thrilled. Since we know how to control the refocus, we can move on and take only a few pixels from the unfocused image and others from the normal one. Thus we can get an aperture of any shape. Yay! (No)

### Many more tricks for video

So, not to move too far away from the photo topic, everyone who interested could check out the links above and below. They contain about half a dozen other interesting applications of a plenoptic camera.

ğŸ¥ [Watch Lytro Change Cinematography Forever](https://www.youtube.com/watch?v=4qXE4sA-hLQ ".block-link")





## Light Field<br><small>More than a photo, less than VR</small>

Usually, the explanation of plenoptic starts from light fields. And yes, from the science perspective, the plenoptic camera captures the light field, not just the photo. Plenus comes from the Latin "full", i.e., collecting all the information about the rays of light. Just like a Parliament plenary session.

Let's get to the bottom of this to understand what is a light field is and why do we need it.

Traditional photo is two-dimensional. There, where ray hit a sensor will be a pixel on a photo. The camera doesn't give a shit where the ray came from, whether it accidentally fell from aside or was reflected by a lovely lady's ass. The photo captures only the point of intersection of the ray with the surface of the sensor. So it's kinda 2D.

Light field image is the same, but with a new component â€” the origin of the ray. Means, it captures the ray vector in 3D space. Like calculating the lighting of a video game, but the other way around â€” we're trying to catch the scene, not create it. The light field is a set of all the light rays in our scene â€” both coming from the light sources and reflected.

 ![There are a lot of mathematical models of light fields. Here's one of the most representative](https://i.vas3k.ru/86h.png)  

The light field is essentially a visual model of the space around it. We can easily compute any photo within this space mathematically. Point of view, depth of field, aperture â€” all these are also computable. 

I love to draw an analogy with a city here. Photography is like your favourite path from your home to the bar you always remember, while the light field is a map of the whole town. Using the map, you can calculate any route from point A to B. In the same way, knowing the light field, we can calculate any photo.

For an ordinary photo it's an overkill, I agree. But here comes the VR, where the light fields there are one of the most promising areas.

Having a light field model of an object or a room allows you to see this object or a room from any point in space as if everything around is virtual reality. It's no longer necessary to build a 3D-model of the room if we want to walk through it. We can "simply" capture all the rays inside it and calculate a picture of the room. Simply, yeah. That's what we're fighting over.

ğŸ“ [Google AR and VR: Experimenting with Light Fields](https://www.blog.google/products/google-ar-vr/experimenting-light-fields/ ".block-link")

 ![](https://i.vas3k.ru/full/871.gif) 



# Computational Optics



Saying optics, I with the [guys from Stanford](http://graphics.stanford.edu/courses/cs478/lectures/02292012_computational_optics.pdf) mean not only lenses but everything in between the object and sensor. Even the aperture and shutter. Sorry, photography snobs. I feel your pain.





## Multi-camera

.block-side.block-side__right.width-25  ![](https://i.vas3k.ru/851.jpg)  

In 2014, the HTC One (M8) was released and became the first smartphone with two cameras and amusing computational photography [features](https://www.computerworld.com/article/2476104/in-pictures--here-s-what-the-htc-one-s-dual-cameras-can-do.html) such as replacing the background with rain or sparkles.

The race has begun. Everybody started putting two, three, five lenses into their smartphones, trying to argue whether telephoto or wide-angle lens is better. Eventually, we got the [Light L16](https://light.co/camera) camera. 16-lensed, as you can guess.

 ![Light L16](https://i.vas3k.ru/859.jpg) 

L16 was no longer a smartphone, but rather a new kind of pocket camera. It promised to reach the quality of top DSLRs with a high-aperture lens and full-frame sensor while yet fitting into your pocket. The power of computational photography algorithms was the main selling point.

.block-side.block-side__right  ![Telephoto-periscope, P30 Pro](https://i.vas3k.ru/854.jpg)  

It had 16 lenses: 5 x 28mm wide-angle and 5 x 70mm and 6 x 150mm telephoto. Each telephoto was periscope-style, meaning that the light did not flow directly through the lens to the sensor, but was reflected by a mirror inside the body. This configuration made it possible to fit a sufficiently long telephoto into a flat body, rather than stick out a "pipe" from it. Huawei recently did the same thing in the P30 Pro.

Each L16 photo was shot simultaneously on 10 or more lenses, and then the camera combined them to get a 52-megapixel image. According to the creators' idea, simultaneous shooting with several lenses made it possible to catch the same amount of light as with the large digital camera lens, artfully bypassing all the laws of optics. 

Talking of software features, the first version had a depth of field and focus control in post-production. Minimal set. Having photos from different perspectives made it possible to compute the depth of the image and apply a decent software blur. Everything seemed nice on paper, so before the release, everybody even had hope for a bright computing future.

 ![](https://i.vas3k.ru/88y.jpg) 

In March 2018, Light L16 penetrated the market andâ€¦  [miserably failed](https://petapixel.com/2017/12/08/review-light-l16-brilliant-braindead/). Yes, technologically it was in the future. However, at a price of $2000 it had no optical stabilization, so that the photos were always blurred (no wonder with 70-150 mm lenses), the autofocus was tediously slow, the algorithms of combining several pictures gave strange sharpness fluctuations, and there was no use for the camera in the dark, as it had no algorithms such as Google's HDR+ or Night Sight. Modern $500 point-and-shoot cameras with RAW support were able to do it from the start, so sales were discontinued after the first batch.

However, Light did not shut down at this point (hehe, pun). It raised the cash and continues to work on the new version with redoubled force. For instance, their technologies used in the recent [Nokia 9](https://www.nokia.com/phones/en_int/nokia-9-pureview/), which is a terrible dream of trypophobe. The idea is encouraging, so we are waiting for further innovations.

ğŸ¥ [Light L16 Review: Optical Insanity](https://www.youtube.com/watch?v=W3pBp12r-m0 ".block-link")





## <br>Coded Aperture<br><small>Deplur + Depth Map</small>

We're entering the area of telescopes, X-rays, and other fog of war. We won't go deep, but it's safer to fasten your seatbelts. The story of the coded aperture began where it was physically impossible to focus the rays: for gamma and X-ray radiation. Ask your physics teacher; they will explain why.

The essence of the coded aperture is to replace the standard petal diaphragm with a pattern. The position of the holes should ensure that the overall shape is maximally varied depending on the defocus â€” the more diverse, the better. Astronomers invented the whole range of [such patterns](http://ipl.uv.es/?q=es/content/page/ibis-coded-mask) for their telescopes. I'll cite the very classical one here.

 ![](https://i.vas3k.ru/88z.jpg) 

How does this work?

When we focus on the object, everything beyond our depth of field is blurred. Physically, blur is when a lens projects one ray onto several pixels of the sensor due to defocus. So a street lamp turns into a bokeh pancake.

Mathematicians use the term convolution and deconvolution to refer to these operations. Let's remember these words cause they sound cool!

 ![](https://i.vas3k.ru/890.jpg) 

Technically, we can turn any convolution back if we know the kernel. That's what mathematicians say. In reality, we have a limited sensor range and non-ideal lens, so all of our bokeh is far from the mathematical ideal and cannot be fully restored.

ğŸ“ [High-quality Motion Deblurring from a Single Image](http://jiaya.me/papers/deblur_siggraph08.pdf ".block-link")

We can still try if we know the kernel of the convolution. Not gonna keep you waiting â€” the kernel is actually the shape of the aperture. In other words, the aperture makes a mathematical convolution using pure optics.

The problem is that the standard round aperture remains round at any level of blurring. Our kernel is always about the same; it's stable, but not very useful. In case of encoded aperture, rays with different defocus degrees will be encoded with different kernels. Readers with IQ > 150 have already guessed what will happen next.

The only issue remains is to understand which kernel is encoded in each area of the image. You can try it on manually, by testing different kernels and looking where the convolution turns out to be more accurate, but this is not our way. A long time ago, people invented the Fourier transform for this. Don't want to abuse you with calculus, so I'll add a link to my favorite explanation for those who are interested.

ğŸ¥ [But what is the Fourier Transform? A visual introduction](https://www.youtube.com/watch?v=spUNpyF58BY ".block-link")

All you need to know is that the Fourier transform allows you to find out which waves are dominant in the pile of overlapped ones. In the case of music, the Fourier will show the frequency of the notes in the complex chord. In the case of photography, it is the main pattern of overlapping light rays, which is the kernel of the convolution.

Since the form of the coded aperture is always different depending on the distance to the object â€” we can calculate that distance mathematically using only one simple image shot with a regular sensor!

Using the inverse convolution on the kernel, we can restore the blurred areas of the image. Bring back all the scattered pixels.

 ![The convolution kernel is at the top right](https://i.vas3k.ru/872.jpg) 

That's how most deblur tools work. It works even with an average round aperture, yet the result is less accurate.

The downside of the coded aperture is the noise and light loss, which we can't ignore. Lidars and fairly accurate ToF-cameras have wholly negated all the ideas of using a coded aperture in consumer gadgets. If you've seen it somewhere, write in comments.

ğŸ“ [Image and Depth from a Conventional Camera with a Coded Aperture](https://graphics.stanford.edu/courses/cs448a-08-spring/levin-coded-aperture-sig07.pdf ".block-link")
ğŸ“ [Coded Aperture. Computational Photography WS 07/08](https://www.eecs.tu-berlin.de/fileadmin/fg144/Courses/07WS/compPhoto/Coded_Aperture.pdf ".block-link")
ğŸ¥ [Coded aperture projection (SIGGRAPH 2008 Talks)](https://www.youtube.com/watch?v=4kh71S446FM ".block-link")





## Phase Coding (Wavefront Coding)

According to the latest news, the light is half the wave. Coding the aperture, we control the transparency of the lens, means we control the wave amplitude. Besides the amplitude, there is a phase, which can also be coded.

And yes. It can be done with an additional lens, which reverses the phase of light passing through it. Like on the Pink Floyd cover.

 ![](https://i.vas3k.ru/892.jpg) 

Then everything works like any other optical encoding. Different areas of the image encoded in different ways, and we can algorithmically recognize and fix them somehow. To shift the focus, for example.

What is good about phase coding is that we don't lose brightness. All photons reach the sensor, unlike in the coded aperture, where they bump into impenetrable parts of it (after all in the other half of the standards said that light is a particle).

The bad part is that we will always lose sharpness, as even the utterly focused objects will be smoothly blurred in the sensor, and we will have to call Fourier to gather them together for us. I'll attach the link with more detailed description and examples of photos below.

ğŸ“ [Computational Optics by Jongmin Baek, 2012](http://graphics.stanford.edu/courses/cs478/lectures/02292012_computational_optics.pdf ".block-link")





## Flutter Shutter<br><small>Fighting the motion blur</small>

The last thing we can code throughout the path of light to the sensor is the shutter. Instead of usual "open â€” wait â€” close" cycle, we will move the shutter several times per shot to result with the desired shutter speed. Sort of as in a multi-exposure, where one shot is exposed several times.

Let's imagine we decided to take pictures of a fast-moving car at night to see its license plate afterward. We don't have a flash, we can't use slow shutter speed, either we'll blur everything. It is necessary to lower the shutter speed, but so we get to a completely black image, and won't recognize the car. What to do?

It also is possible to take this shot in flutter shutter movements, so that the car smear not evenly, but like a "ladder" with a known interval. Thus, we encode the blur with a random sequence of open-close of the shutter, and we can try to decode it with the same inverse convolution. Appears it works much better than trying to get back pixels, evenly blurred with long shutter speed.

 ![](https://i.vas3k.ru/893.jpg) 

There are several algorithms for that. For the hardcore details, I'll again include links to some smart Indian guys' work.

ğŸ“ [Coded exposure photography: motion deblurring using fluttered shutter](http://www.cs.cmu.edu/~ILIM/projects/IM/aagrawal/sig06/CodedExposureLowres.pdf ".block-link")
ğŸ¥ [Flutter Shutter Coded Filter](https://www.youtube.com/watch?v=gGvvqj-lF5o ".block-link")




# Computational Lighting



Soon we'll go so goddamn crazy, so we'd want to control the lighting after the photo was taken too. To change the cloudy weather to a sunny one, or to change the lights on a model's face after shooting. Now it seems a bit wild, but let's talk again in ten years.

We've already invented a dumb device to control the light â€” a flash. They have come a long way: from the large lamp boxes that helped avoid the technical limitations of early cameras, to the modern LED flashes that spoil our pictures, so we mainly use them as a flashlight.

 ![](https://i.vas3k.ru/full/87b.gif)  





## Programmable Flash

It's been a long time since all smartphones switched to Dual LED flashes â€” a combination of orange and blue LEDs with brightness being adjusted to the color temperature of the shot. In the iPhone, for example, it's called True Tone and controlled by a small piece of code with a hacky formula. Even developers are not allowed to control it.

ğŸ“ [Demystifying iPhoneâ€™s Amber Flashlight](https://medium.com/@thatchaponunprasert/demystifying-iphones-amber-flashlight-519352db10bd ".block-link")

 ![](https://i.vas3k.ru/87k.jpg) 

Then we started to think about the problem of all flashes â€” the overexposed faces and foreground. Everyone did it in their own way. iPhone got [Slow Sync Flash](https://www.reddit.com/r/iphone/comments/71myyp/a_feature_from_the_new_new_iphone_a_few_talk_about_is/), which made camera artificially increase shutter speed in the dark. Google Pixel and other Android smartphones start using their depth sensors to combine images with and without flash, quickly made one by one. The foreground was taken from the photo without the flash when the background remained illuminated.

 ![](https://i.vas3k.ru/86r.jpg) 

The further use of a programmable multi-flash is vague. The only interesting application was found in computer vision, where it was used once in assembly scheme (like for Ikea book shelves) to detect the borders of objects more accurately. See the article below.

ğŸ“ [Non-photorealistic Camera:
Depth Edge Detection and Stylized Rendering using Multi-Flash Imaging](https://www.eecis.udel.edu/~jye/lab_research/SIG04/SIG_YU_RASKAR.pdf ".block-link")





## Lightstage

Light is fast. It's always made light coding an easy thing to do. We can change the lighting a hundred times per shot and still not get close to its speed. That's how Lighstage was created back in 2005. 

 ![](https://i.vas3k.ru/86d.jpg) 

ğŸ¥ [Lighstage demo video](https://www.youtube.com/watch?v=wT2uFlP0MlU)

The essence of the method is to highlight the object from all possible angles in each shot of a real 24 fps movie. To get this done, we use 150+ lamps and a high-speed camera that captures hundreds of shots with different lighting conditions per shot.

A similar approach is now used when shooting mixed CGI graphics in movies. It allows you to fully control the lighting of the object in the post-production, placing it in scenes with absolutely random lighting. We just grab the shots illuminated from the required angle, tint them a little, done.

 ![](https://i.vas3k.ru/86s.jpg) 

 ![](https://i.vas3k.ru/86e.jpg) 

Unfortunately, it's hard to do it on mobile devices, but probably someone will like the idea. I've seen the app from guys who shot a 3D face model, illuminating it with the phone flashlight from different sides.





## Lidar and Time-of-Flight Camera

Lidar is a device that determines the distance to the object. Thanks to a recent hype of self-driving cars, now we can find a cheap lidar on any dumpster. You've probably seen these rotating thingys at their roof? These are lidars.

We still can't fit a laser lidar into a smartphone, but we can go with its younger brother â€” [time-of-flight camera](https://en.m.wikipedia.org/wiki/Time-of-flight_camera). The idea is ridiculously simple â€” a special separate camera with an LED-flash above it. The camera measures how quickly the light reaches the objects and creates a depth map of the image.

 ![](https://i.vas3k.ru/868.jpg) 

The accuracy of modern ToF cameras is about a centimeter. The latest Samsung and Huawei top models use them to create a bokeh map and for better autofocus in the dark. The latter, by the way, is quite good. I wish everybody had one.

Knowing the exact depth of field will be useful in the coming era of augmented reality. It will be much more accurate and effortless to shoot at the surfaces with lidar to make the first mapping in 3D than analyzing camera images.





## Projector Illumination

To finally get serious about the computational lighting, we have to switch from regular LED flashes to projectors â€” devices that can project a 2D picture on a surface. Even a simple monochrome grid will be a good start for smartphones.

The first benefit of the projector is that it can illuminate only the part of the image that needs to be illuminated. No more burnt faces in the foreground. Objects can be recognized and ignored, just like laser headlights of modern car don't blind the oncoming drivers but illuminate pedestrians. Even with the minimum resolution of the projector, such as 100x100 dots, the possibilities are exciting.

 ![Today, you can't surprise a kid with a car with a controllable light](https://i.vas3k.ru/86i.jpg) 

The second and more realistic use of the projector is to project an invisible grid on a scene to detect its depth map. With a grid like this, you can safely throw away all your neural networks and lidars. All the distances to the objects in the image now can be calculated with the simplest computer vision algorithms. It was done in Microsoft Kinect times (rest in peace), and it was great.

Of course, it's worth to remember here the Dot Projector for Face ID on iPhone X and above. That's our first small step towards projector technology, but quite a noticeable one.

 ![Dot Projector in iPhone X](https://i.vas3k.ru/86j.jpg) 




# The Future of Photography<br><small>Controlling the 3D scene and Augmented Reality</small>



.block-side.block-side__right.width-25  ![](https://j.gifs.com/wVrzx8.gif)  

It's time to reflex a bit. Observing what major technology companies are doing, it becomes clear that our next 10 years will be tightly tied to augmented reality. Today AR still looks like a toy to play [with 3D wifey](https://youtu.be/p9oDlvOV3qs?t=161), to [try on sneakers](https://www.youtube.com/watch?v=UmJriqzDUTo), to see [how the makeup looks](https://www.youtube.com/watch?v=dpSP6ZM5XGo), or to train [the U.S. Army](https://www.youtube.com/watch?time_continue=87&v=x8p19j8C6VI). Tomorrow we won't even notice we're using it every day. Dense flows of cash in this area are already felt from the Google and Nvidia offices.

For photography, AR means the ability to control the 3D scene. Scan the area, like smartphones with [Tango](https://en.wikipedia.org/wiki/Tango_%28platform%29) do, add new objects, like in [HoloLenz](https://youtu.be/e-n90xrVXh8?t=314), all such things. Don't worry about the poor graphics of modern AR-apps. As soon as game dev companies invade the area with their battle royales, everything becomes much better than PS4.

.block-media.block-media__2  ![](https://i.vas3k.ru/87f.jpg)  ![](https://i.vas3k.ru/87h.jpg)  

% By [Defected Pixel](https://vk.com/pxirl)

Remember that epic [fake Moon Mode](https://www.androidauthority.com/huawei-p30-pro-moon-mode-controversy-978486/) presented by Huawei? If you missed it: when Huawei camera detects you're going to take a photo of moon, it puts a pre-prepared high-resolution moon picture on top of your photo. Because it looks cooler, indeed! True Chinese cyberpunk.

 ![Life goal: be able to bend the truth like Huawei](https://i.vas3k.ru/869.jpg) 

When all the jokes were joked in twitter, I thought about that situation â€” Huawei gave people exactly what they promised. The moon was real, and the camera lets you shoot it THIS awesome. No deception. Tomorrow, if you give people the opportunity to replace the sky on their photos with beautiful sunsets, half the planet will be amazed.

> In the future, machines will be "finishing up" and re-painting our photos for us

Pixel, Galaxy and other Android-phones have some stupid AR-mode today. Some let you add cartoon characters to take photos with them, others spread emojis all over the room, or put a mask on your face just like in a Snapchat. 

These are just our first naive steps. Today, Google camera has Google Lens, that finds information about any object you point your camera at. Samsung does the same with Bixby. For now, these tricks are only made to humiliate the iPhone users, but it's easy to imagine the next time you're taking a pic with the Eiffel Tower, your phone says: *you know, your selfie is shit. I'll put a nice sharp picture of the tower in the background, fix your hair, and remove a pimple above your lip. If you plan to post it to Instagram, VSCO L4 filter will work the best for it. You're welcome, leather bastard.*

After a while, the camera will start to replace the grass with greener one, your friends with better ones, and boobs with bigger ones. Or something like that. A brave new world.

 ![](https://i.vas3k.ru/894.jpg) 

In the beginning it's gonna look ridiculous. Probably even terrible. The photo-aesthetes will be enraged, the fighters for natural beauty will launch a company to ban neural networks usage, but the mass audience will be delighted.

Because photography always was just a way to express and share emotions. Every time there is a tool to express more vividly and effectively, everyone starts using it â€” emoji, filters, stickers, masks, audio messages. Some will already find the list disgusting, but it can be easily continued. 

Photos of the "objective reality" will seem as boring as your great-grandmother's pictures on the chair. They won't die but become something like paper books or vinyl records â€” a passion of enthusiasts, who see a special deep meaning in it. "*Who cares of setting up the lighting and composition when my phone can do the same"*. That's our future. Sorry.

The mass audience doesn't give a shit about objectivity. It needs algorithms to make their faces younger, and vacations cooler than their coworker or neighbor. The augmented reality will re-draw the reality for them, even with a higher level of detail than it really is. It may sound funny, but we'll start to improve the graphics in the real world. 

And yes, as it always does, it all starts with teenagers with their "strange, stupid hobbies for idiots". That's what happens all the time. When you stop understanding something â€” this IS the future.

.block-iframe <iframe width="100%" height="320" src="https://www.youtube.com/embed/YJg02ivYzSs" frameborder="0" allowfullscreen=""></iframe> 

ğŸ’ [Augmented Reality](https://vas3k.com/blog/augmented_reality/ ".block-link")





# Conclusion

.block-text.nocomments

Throughout history, each human technology becomes more advanced as soon as it stops copying living organisms. Today, it is hard to imagine a car with joints and muscles instead of wheels. Planes with fixed wings fly 800+ km/h â€” birds don't even try. There are no analogs to the computer processor in nature at all.

The most exciting part of the list is what's not in it. Camera sensors. We still haven't figured out anything better than imitating the eye structure. The same crystalline lens and a set of RGGB-cones as retina has.

Computational photography has added a "brain" to this process. A processor that handles visual information not only by reading pixels through the optic nerve but also by complementing the picture based on its experience. Yes, it opens up a lot of possibilities for us today, but there is a hunch we're still trying to wave with hand-made wings instead of inventing a plane. One that will leave behind all these shutters, apertures, and Bayer filters.

The beauty of the situation is that we can't even imagine today what it's going to be.

And it's wonderful.

==========
Product challenges
- latency optimization
- quality

projection:

super-res zoom:
- multi-frames super resolution
- natural physiological tremor
- tree? solution: kernel regression and interpolation

product challenges
- moving object
- low light / noise scenes
- extreme aliasing

inject motion by the optical image stabilizer(OIS)